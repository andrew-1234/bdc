---
title: "Standardization and integration of different datasets"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Standardization and integration of different datasets}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  echo = TRUE,
  warning = FALSE,
  eval = T
)
```

## Introduction

The first step of the BDC workflow handles the harmonization of heterogeneous datasets in a standard format simply and efficiently. How is this accomplished? Basically, by replacing the headers of original datasets with standardized terms. To do so, you have to fill out a **configuration table** to indicate which field names (i.e., column headers) of each original dataset match a list of **Darwin Core standard terms**.

Once standardized, datasets are then integrated into a standardized database having a minimum set of terms required for sharing biodiversity data and metadata across a wide variety of biodiversity applications ([Simple Darwin Core standards](https://dwc.tdwg.org/terms/)).

<br/>

<img src="https://img.icons8.com/windows/96/000000/box-important--v1.png" width="40"/> **Important**:

-   Original datasets must be formatted in comma-separated format (**.csv**);
-   When filling out the configuration table, please provide the **exact** name of a column of the original dataset and the **full path** of each original dataset;
-   Names of original datasets without a corresponding DarwinCore term must be filled as **NA** (not available);
-   The workflow is **adjustable** so that you can insert other fields in the configuration table according to your needs. In such cases, we strongly recommend that the added terms follow the Darwin Core standards.

## Installation

You can install the released version of BDC from [github](https://github.com/brunobrr/bdc) with:

```{r, message=FALSE, warning=FALSE}
if (!require("remotes")) install.packages("remotes")
if (!require("bdc")) remotes::install_github("brunobrr/bdc")
```

First, let's create folders to save the results.

```{r}
bdc::bdc_create_dir()
```

## Read the configuration table

Read an example of the **configuration table**. You can download the table by clicking on "CSV" button. We demonstrate the usefulness of the workflow using a database of terrestrial plant species occurring in Brazil. This database contains 9.000 records compiled from nine different sources (see below).

```{r message=FALSE, warning=FALSE, eval=FALSE}
metadata <- readr::read_csv(system.file("extdata/Config/DatabaseInfo.csv", package = "bdc"))
```

```{r message=FALSE, warning=FALSE, eval=TRUE, echo = FALSE}
metadata <- readr::read_csv(here::here("inst", "extdata", "Config", "DatabaseInfo.csv"))
```

<br/> <br/>

```{r echo = FALSE, message=FALSE, warning=FALSE, eval=TRUE}
if (!require("DT")) install.packages('DT')

DT::datatable(
  metadata, class = 'stripe', extensions = c('FixedColumns', 'Buttons'),
  options = list(
    #dom = 't',
    dom = 'Bfrtip',
    scrollX = TRUE,
    pageLength = 5,
    buttons = c('copy', 'csv', 'print'),
    fixedColumns = list(leftColumns = 2),
    editable = 'cell'
  )
)
```

<br/>

The standardized database embodies information on species taxonomy, geolocation, date of collection, and other relevant context information. Each field is classified in three categories according to its importance to run the workflow: i) **required**, i.e., the minimum information necessary to run the workflow, ii) **recommended**, i.e., not mandatory but having important details on species records, and iii) **additional**, i.e., information potentially useful for detailed data analyses.

Below are listed the specifications of each field of the configuration table:

-   **Field**: Name of the fields in *DatabaseInfo.csv* to be filled in.
-   **Category**: Classification of each field in *DatabaseInfo.csv*. *required (RQ)*, i.e., the minimum information necessary to run the workflow, ii) *recommended (RE)*, i.e., not mandatory but having important details on species records, and iii) *additional (AD)*, i.e., information potentially useful for detailed data analyses. As a general guidance, be careful to include all *required* fields and supply as many recommended and additional fields as possible.
-   **Description**: Description about the content of the specified field in the original database.
-   **Type**: Type of content data on the specified field in the original database.
-   **Example**: An example of a single content on the specified field in the original database.

<br/> <br/>

```{r echo=F, message=FALSE, warning=FALSE}
config_description <-
  readr::read_csv(here::here("inst", "extdata", "Config", "DatabaseInfo_description.csv"))

DT::datatable(
  config_description, class = 'stripe', extensions = c('FixedColumns', 'Buttons'),
  options = list(
    #dom = 't',
    dom = 'Bfrtip',
    scrollX = TRUE,
    pageLength = 5,
    buttons = c('copy', 'csv', 'print'),
    fixedColumns = list(leftColumns = 2),
    editable = 'cell'
  )
)
```

## Standardization and integration of datasets

Note that the standardized database integrating all dataset is saved in the folder "Output/Intermediate" as *"00_merged_database.qs"*. The database is saved with a **"qs" extension**, a format useful for quickly saving and reading large databases. This file can be read using the function "qread" from the "qs" package (see below).

```{r message=F, warning=F}
bdc_standardize_datasets(metadata = metadata)
```

```{r message=F, warning=F, echo = FALSE}
database <-
  here::here("Output", "Intermediate", "00_merged_database.qs") %>%
  qs::qread()
```

<br/>

An example of a standardized database containing the required field to run the BDC workflow.

<br/>

```{r echo=F, message=FALSE, warning=FALSE}
DT::datatable(
  database, class = 'stripe', extensions = 'FixedColumns',
  options = list(
    pageLength = 3,
    dom = 'Bfrtip',
    scrollX = TRUE,
    fixedColumns = list(leftColumns = 2)
  )
)
```
